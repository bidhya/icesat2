{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download ATL06 Data from NSIDC server\n",
    "Should work with any random area (either shapefile or coordinates of box)  \n",
    "Make changes to the codes following number 1 to 6 to fulfill your requirements  \n",
    "\n",
    "## Inputs:\n",
    "    - define the folder to download and process files\n",
    "    - shapefile with defined projection (can be be in any system)\n",
    "    - or coordinates of box [lowerleft, upper right] or coordinates in lat/lon coordinate system\n",
    "    \n",
    "Last Update: Sept 21, 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "from datetime import timedelta\n",
    "\n",
    "#import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, Polygon #, mapping\n",
    "#from shapely.geometry.polygon import orient\n",
    "from statistics import mean\n",
    "import h5py\n",
    "import json\n",
    "import zipfile\n",
    "import io\n",
    "\n",
    "import requests\n",
    "import getpass\n",
    "import socket\n",
    "#from requests.auth import HTTPBasicAuth\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```If you get any error, that means some packages are missing, most likely geopandas and h5py   ```\n",
    "Install them from Anaconda navigator  \n",
    "or manually from command prompt/terminal as such  \n",
    "- conda install geopandas\n",
    "- conda install h5py\n",
    "\n",
    "If there is some error in installation, it means package not available in default anaconda package. use the following command  \n",
    "- conda install h5py -c conda-forge\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Choose output Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Housekeeping: This is where files will be downloaded and all ouputs saved\n",
    "# icesat2_path = 'D:/wspace/icesat2/problematic' #CHANGE THIS\n",
    "icesat2_path = 'test_data' #CHANGE THIS\n",
    "if not os.path.exists(icesat2_path):\n",
    "    print(f'Create Output Directory : {icesat2_path}')\n",
    "    os.makedirs(icesat2_path) #exist_ok=True to prevent complaint if directory exist\n",
    "    os.mkdir(f'{icesat2_path}/downloads')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Either give a full path to shapefile or replace the corner coordinates below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No shapefile give, replace the lower-left and upper-right coordinates to define the box\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Either give the full path to outline as polygon shapefile\n",
    "shp = ''#'gis/aoi_greenland/aoi_for_icesat.shp' #insert full path to shapefile if you have one\n",
    "if shp:\n",
    "    gdf = gpd.read_file(shp)\n",
    "    #gdf = get_rema_strip_polygon(strip_folder, 'arctic')\n",
    "    # Convert to Latlong\n",
    "    gdf = gdf.to_crs('epsg:4326') #use this format if error due to old geopandas version {'init': 'epsg:4326'}\n",
    "else:\n",
    "    print('No shapefile give, replace the lower-left and upper-right coordinates to define the box')\n",
    "    # Or Digitize the outline of shapefile (replace the coordinates for your area here)\n",
    "    #POLYGON ((-175.6012946450045 -78.04174089275783, 180 -78.07672014316189, 180 -77.16663242606151, -175.9143832200251 -77.13419881448696, -175.6012946450045 -78.04174089275783))\n",
    "    x0 = -175.6012946450045 #100 # -147.5\n",
    "    y0 = -78.04174089275783#-66 #64.2\n",
    "    x1 = -180 #101 #-146.5\n",
    "    y1 = -77.16663242606151 #-67 #65.2\n",
    "    geom = Polygon([(x0, y0), (x1, y0), (x1, y1), (x0, y1)])\n",
    "    gdf = gpd.GeoSeries(geom, crs='epsg:4326')\n",
    "# Save for future use (but not required for script to work)    \n",
    "gdf.to_file(f'{icesat2_path}/outline.shp')\n",
    "shp_json = gdf.to_json() #use json as shapefile, or kml too, shp file not yet tried\n",
    "gdf.plot()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import hvplot\n",
    "import hvplot.pandas\n",
    "import geoviews as gv\n",
    "\n",
    "# # Since geoseries can't be plotted by holoviews, convert to geodataframe and define the geometry\n",
    "# gdf1 = gpd.GeoDataFrame(gdf)\n",
    "# gdf1.columns = ['geometry']\n",
    "\n",
    "base = gv.tile_sources.ESRI\n",
    "\n",
    "strips = gdf.hvplot(geo=True, alpha=0.5, c=None)\n",
    "base * strips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'-180.0,-78.04174089275783,-175.6012946450045,-77.16663242606151'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#% To use Bounding Box for subsetting\n",
    "bounding_box = ','.join(map(str, list(gdf.total_bounds)))\n",
    "bounding_box\n",
    "# Bounding box subsetting (bbox) in same format as bounding_box\n",
    "#bbox = bounding_box #not used in post"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Temporal Subsetting\n",
    "Replace Start and End date time in the following string format by replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-01T00:00:00Z,2020-01-05T23:59:59Z\n"
     ]
    }
   ],
   "source": [
    "start_date = '2020-01-01' #yyyy-MM-dd format '2018-10-14' #'2019-04-01' # start of ATL06 2018/10/14\n",
    "start_time = '00:00:00' #HH:mm:ss format\n",
    "end_date = '2020-01-05' #'2019-09-30'\n",
    "end_time = '23:59:59'\n",
    "temporal = start_date + 'T' + start_time + 'Z' + ',' + end_date + 'T' + end_time + 'Z'\n",
    "print(temporal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# No need to change anything below except item 4 (your authentication)\n",
    "But observe the size of our ouputs and modify spatial/temporal extent accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Granules search parameters the NSIDC Server\n",
    "short_name = 'ATL06'\n",
    "latest_version = '004' #directly without using the cmr search to simplify the script\n",
    "search_params = {\n",
    "        'short_name': short_name,\n",
    "        'version': latest_version,\n",
    "        'temporal': temporal,\n",
    "        'page_size': 100,\n",
    "        'page_num': 1,\n",
    "        'bounding_box': bounding_box\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query number of granules using our (paging over results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "Number of Granules = 2\n"
     ]
    }
   ],
   "source": [
    "granule_search_url = 'https://cmr.earthdata.nasa.gov/search/granules'\n",
    "headers={'Accept': 'application/json'} #if not defined in first block to get the token\n",
    "granules = []\n",
    "count = 0\n",
    "while True:\n",
    "    print(count)\n",
    "    response = requests.get(granule_search_url, params=search_params, headers=headers)\n",
    "    results = json.loads(response.content)\n",
    "    if len(results['feed']['entry']) == 0:\n",
    "        # Out of results, so break out of loop\n",
    "        break\n",
    "    # Collect results and increment page_num\n",
    "    granules.extend(results['feed']['entry'])\n",
    "    search_params['page_num'] += 1\n",
    "    count +=1\n",
    "# Get number of granules over my area and time of interest\n",
    "print(f'Number of Granules = {len(granules)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Size of Granules:  11.290370941099999 MB\n"
     ]
    }
   ],
   "source": [
    "granule_sizes = [float(granule['granule_size']) for granule in granules]\n",
    "print('Total Size of Granules: ', sum(granule_sizes), 'MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Choose the authentication mechanism\n",
    "username, password, and the token (by running the token code below (item 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success getting authentication\n"
     ]
    }
   ],
   "source": [
    "# Query service capability URL : seems only to see what service is availabe\n",
    "from xml.etree import ElementTree as ET\n",
    "capability_url = f'https://n5eil02u.ecs.nsidc.org/egi/capabilities/{short_name}.{latest_version}.xml'\n",
    "#print(capability_url)\n",
    "\n",
    "#Create session to store cookie and pass credentials to capabilities url\n",
    "email=''\n",
    "try:\n",
    "    from icesat2_download import get_api_key\n",
    "    uid, pswd, token = get_api_key()\n",
    "    print(\"Success getting authentication\")\n",
    "except:\n",
    "    # Input explicitly here\n",
    "    uid = ''#'Replace with your username'\n",
    "    email = '' #if you provide your email, you will receive email for every order; \n",
    "    pswd = ''#'Repalce with your password'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Request token from Common Metadata Repository using Earthdata credentials\n",
    "Run this block of code only once, note the printed token, and replace the token in the next block of code"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "token_api_url = 'https://cmr.earthdata.nasa.gov/legacy-services/rest/tokens'\n",
    "# hostname and ip may be not used!\n",
    "hostname = socket.gethostname() # get my personal computer name, eg staff-xxx-xxx; 'STAFF-BY-xx'\n",
    "ip = socket.gethostbyname(hostname)\n",
    "data = {\n",
    "    'token': {\n",
    "        'username': uid,\n",
    "        'password': pswd,\n",
    "        'client_id': 'NSIDC_client_id',\n",
    "        'user_ip_address': ip\n",
    "    }\n",
    "}\n",
    "headers={'Accept': 'application/json'}\n",
    "response = requests.post(token_api_url, json=data, headers=headers)\n",
    "token = json.loads(response.content)['token']['id']\n",
    "print(token) #use this token for a month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pswd = getpass.getpass('Earthdata Login password: ')\n",
    "# token = '' #'Replace this with the output above' \n",
    "\n",
    "session = requests.session()\n",
    "s = session.get(capability_url)\n",
    "\n",
    "response = session.get(s.url,auth=(uid,pswd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage = ''\n",
    "# Temporal subsetting KVP\n",
    "timevar = temporal # this seemed to work as well, when used temporal below directly\n",
    "# Request data from the NSIDC data access API.\n",
    "'''the API is structured as a URL with a base plus individual key-value-pairs (KVPs) separated by ‘&’. \n",
    "The base URL of the NSIDC API is: '''\n",
    "base_url = 'https://n5eil02u.ecs.nsidc.org/egi/request' ##Set NSIDC data access base URL\n",
    "\n",
    "# Set number of granules requested per order, which we will initially set to 10.\n",
    "page_size = 10\n",
    "#Determine number of pages basd on page_size and total granules. Loop requests by this value\n",
    "page_num = math.ceil(len(granules)/page_size)\n",
    "#Set request mode. \n",
    "request_mode = 'async' #with synchronous, there is possibility of timeout errors\n",
    "\n",
    "#Create config dictionary\n",
    "config_params = {\n",
    "    'request_mode': request_mode, \n",
    "    'page_size': page_size,  \n",
    "    'token': token, \n",
    "    'email': email,   \n",
    "}\n",
    "\n",
    "#timevar replaced with temporal: and it seems to work fine\n",
    "custom_params = {\n",
    "    'time': temporal,\n",
    "    'Coverage': coverage, \n",
    "    }\n",
    "# Creating final request parameter dictionary with search, config, and customization parameters.\n",
    "subset_request_params = {**search_params, **config_params, **custom_params}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This block of code will download the data  \n",
    "- may take a few minutes\n",
    "- you will receive email if you provided one\n",
    "- may print some error: these are mostly system error on NSIDC part (at least thats what I learned for Any at the AGU meeting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Order:  1\n",
      "order ID:  5000001165555\n",
      "status URL:  https://n5eil02u.ecs.nsidc.org/egi/request/5000001165555\n",
      "Data request  1  is submitting...\n",
      "Initial request status is  processing\n",
      "Status is not complete. Trying again.\n",
      "Retry request status is:  complete\n",
      "Beginning download of zipped output...\n",
      "Data request 1 is complete.\n"
     ]
    }
   ],
   "source": [
    "#%% Choose method to make a request\n",
    "method = 'post' #'get' 'post'\n",
    "for i in range(page_num):\n",
    "    page_val = i + 1\n",
    "    print('Order: ', page_val)\n",
    "    if method == 'post':\n",
    "        # We are sending shapefile (in KML format) to the server\n",
    "        subset_request_params.update( {'page_num': page_val})\n",
    "        # Post polygon to API endpoint for polygon subsetting to subset based on original, non-simplified KML file\n",
    "        #shape_post = {'shapefile': open(kml_filepath, 'rb')}\n",
    "        shape_post = {'shapefile': shp_json}\n",
    "        \n",
    "        request = session.post(base_url, params=subset_request_params, files=shape_post, auth=(uid,pswd))\n",
    "\n",
    "    # Raise bad request: Loop will stop for bad response code.\n",
    "    request.raise_for_status()\n",
    "    #print('Order request URL: ', request.url)\n",
    "    esir_root = ET.fromstring(request.content)\n",
    "    #print('Order request response XML content: ', request.content)\n",
    "\n",
    "    #Look up order ID\n",
    "    orderlist = []\n",
    "    for order in esir_root.findall(\"./order/\"):\n",
    "        orderlist.append(order.text)\n",
    "    orderID = orderlist[0]\n",
    "    print('order ID: ', orderID)\n",
    "\n",
    "    #Create status URL\n",
    "    statusURL = base_url + '/' + orderID\n",
    "    print('status URL: ', statusURL)\n",
    "\n",
    "    #Find order status\n",
    "    request_response = session.get(statusURL)\n",
    "    #logging.info(f'Order Status HTTP response: {request_response.status_code}')\n",
    "\n",
    "    # Raise bad request: Loop will stop for bad response code.\n",
    "    request_response.raise_for_status()\n",
    "    request_root = ET.fromstring(request_response.content)\n",
    "    statuslist = []\n",
    "    for status in request_root.findall(\"./requestStatus/\"):\n",
    "        statuslist.append(status.text)\n",
    "    status = statuslist[0]\n",
    "    print('Data request ', page_val, ' is submitting...')\n",
    "    print('Initial request status is ', status)\n",
    "\n",
    "    #Continue loop while request is still processing\n",
    "    while status == 'pending' or status == 'processing': \n",
    "        print('Status is not complete. Trying again.')\n",
    "        sleep_sec = 20\n",
    "        time.sleep(sleep_sec)\n",
    "        loop_response = session.get(statusURL)\n",
    "        # Raise bad request: Loop will stop for bad response code.\n",
    "        loop_response.raise_for_status()\n",
    "        loop_root = ET.fromstring(loop_response.content)\n",
    "        #find status\n",
    "        statuslist = []\n",
    "        for status in loop_root.findall(\"./requestStatus/\"):\n",
    "            statuslist.append(status.text)\n",
    "        status = statuslist[0]\n",
    "        print('Retry request status is: ', status)\n",
    "        if status == 'pending' or status == 'processing':\n",
    "            continue\n",
    "    #Order can either complete, complete_with_errors, or fail:\n",
    "    # Provide complete_with_errors error message:\n",
    "    if status == 'complete_with_errors' or status == 'failed':\n",
    "        messagelist = []\n",
    "        for message in loop_root.findall(\"./processInfo/\"):\n",
    "            messagelist.append(message.text)\n",
    "        logging.error(f'error messages: {messagelist}')\n",
    "        #pprint.pprint(messagelist)\n",
    "\n",
    "    # Download zipped order if status is complete or complete_with_errors\n",
    "    #'https://n5eil02u.ecs.nsidc.org/esir/5000000402535/166238470/\n",
    "    #processed_ATL06_20181102070512_05290110_002_01.h5\n",
    "    if status == 'complete' or status == 'complete_with_errors':\n",
    "        downloadURL = 'https://n5eil02u.ecs.nsidc.org/esir/' + orderID + '.zip'\n",
    "        print('Beginning download of zipped output...')\n",
    "        zip_response = session.get(downloadURL)\n",
    "        # Raise bad request: Loop will stop for bad response code.\n",
    "        zip_response.raise_for_status()\n",
    "        with zipfile.ZipFile(io.BytesIO(zip_response.content)) as z:\n",
    "            z.extractall(f'{icesat2_path}/downloads')\n",
    "        print('Data request', page_val, 'is complete.')\n",
    "    else: print('Request failed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further processing\n",
    "a. Extract the zip files  \n",
    "b. Parse the HDF files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extract the downloaded HDF files and cleanup the downloaded folder\n",
    "def move_files_from_order(icesat2_path):\n",
    "    ''' Extract files from downloaded subfoder [ie one by orderid] \n",
    "        and move all hdf files in one location\n",
    "    '''\n",
    "    hdf_path = icesat2_path\n",
    "    for root, dirs, files in os.walk(f'{icesat2_path}/downloads', topdown=False):\n",
    "        for f in files:\n",
    "            if f.endswith('h5'):\n",
    "                try:\n",
    "                    shutil.move(os.path.join(root, f), hdf_path)\n",
    "                except:\n",
    "                    logging.error(\"Extraction Error (SHUTIL): perhaps file already exist\")\n",
    "    #os.rmdir(f'{icesat2_path}/downloads') #for empty\n",
    "    shutil.rmtree(f'{icesat2_path}/downloads')\n",
    "    \n",
    "# Cleanup : move the hdf files to base folder and deleted the \"downloads\" folder\n",
    "move_files_from_order(icesat2_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert/Parse the HDF files to csv file/shapefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['processed_ATL06_20200104063555_01300612_003_01.h5',\n",
       " 'processed_ATL06_20200104172232_01370610_003_01.h5',\n",
       " 'processed_ATL06_20200104172232_01370610_004_01.h5',\n",
       " 'processed_ATL06_20200104063555_01300612_004_01.h5']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = os.listdir(icesat2_path)\n",
    "hdf_files = [f for f in files if f.endswith('.h5')]\n",
    "hdf_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% To Parse hdf file and convert csv and shapefile for analysis/visualization\n",
    "from astropy.time import Time\n",
    "def gps2dyr(time, offset = 0):\n",
    "    \"\"\" Converte GPS time to decimal years. Helper function\"\"\"\n",
    "    time = time + offset\n",
    "    gps_time = Time(time, format='gps')#.decimalyear\n",
    "    iso_time = Time(gps_time, format='iso')\n",
    "    iso_time = iso_time.value\n",
    "    # Conver to pandas datetime [not sure if it is utc]\n",
    "    dt = pd.to_datetime(iso_time)\n",
    "    return dt\n",
    "\n",
    "def read_atl06(icesat2_path, gis_output='shp'):\n",
    "    \"\"\" Read 1 ATL06 file and output 6 reduced files.     \n",
    "        Extract variables of interest and separate the ATL06 file \n",
    "        into each beam (ground track) and ascending/descending orbits.\"\"\"\n",
    "    files = os.listdir(icesat2_path)\n",
    "    hdf_files = [f for f in files if f.endswith('.h5') and 'ATL06' in f]\n",
    "    logging.info(f'Number of HDF files: {len(hdf_files)}')\n",
    "    for f in hdf_files:\n",
    "        logging.info(f'Parsing HDF file: {f}')\n",
    "        hdf_path = f'{icesat2_path}/{f}'\n",
    "        res_dict = {}\n",
    "        meta_dict = {} #These will hold metadata required for scalars per ground-track\n",
    "        group = ['gt1l', 'gt1r', 'gt2l', 'gt2r', 'gt3l', 'gt3r']\n",
    "        qual_str_count = ''\n",
    "        # Loop trough beams\n",
    "        # Perhaps read file first, then loop through groups; should be faster\n",
    "        with h5py.File(hdf_path, 'r') as fi:\n",
    "            # subset group based on data\n",
    "            group = [g for g in list(fi.keys()) if g in group]\n",
    "            group1 = len(group)\n",
    "            group = [g for g in group if 'land_ice_segments' in fi[f'/{g}']]\n",
    "            group2 = len(group)\n",
    "            if group2<group1:\n",
    "                print(f'Non-empty groups: {group2}/{group1}')\n",
    "            # NB: Assert if at least one group present else may be error due to enumeration\n",
    "            if len(group) == 0:\n",
    "                print('No any Group! ie, no land ice data. Halting further processing for this granule')\n",
    "                continue\n",
    "            for k,g in enumerate(group):\n",
    "                #if 'land_ice_segments' in fi[f'/{g}']:\n",
    "                # 1) Read in data for a single beam #\n",
    "                lat = fi[f'/{g}/land_ice_segments/latitude'][:]\n",
    "                lon = fi[f'/{g}/land_ice_segments/longitude'][:]\n",
    "                h_li = fi[f'/{g}/land_ice_segments/h_li'][:] #nan\n",
    "                #s_li = fi[f'/{g}/land_ice_segments/h_li_sigma'][:] #nan\n",
    "                t_dt = fi[f'/{g}/land_ice_segments/delta_time'][:]\n",
    "                q_flag = fi[f'/{g}/land_ice_segments/atl06_quality_summary'][:]\n",
    "                t_ref = fi['/ancillary_data/atlas_sdp_gps_epoch'][:] #scalar 1 value; required for offset\n",
    "                \n",
    "                meta_dict['t_ref'] = t_ref #dictionary of metadata (will be used in future to investigate data)\n",
    "                # 2) Make Pandas dataframe\n",
    "                df = pd.DataFrame({'lon':lon, 'lat':lat, 'h_li': h_li, 'q_flag':q_flag, 't_dt':t_dt})\n",
    "                #Convert GPS time to actual time using function\n",
    "                df['t_dt'] = df['t_dt'].apply(gps2dyr, offset=t_ref[0])\n",
    "                # Fill Nans for na-data and drop\n",
    "                df.loc[df.h_li>3e38, 'h_li'] = np.nan\n",
    "                df = df.dropna()\n",
    "                if len(df)>0:\n",
    "                    # Assemble ground track into a dictionary, later we convert to csv and shp through df\n",
    "                    res_dict[g] = df\n",
    "            #----------------------------------------------------------------------------------------------\n",
    "            # Now that ATL06 data from separate ground tracks are in one dict, merge it to df and save to csv/shp\n",
    "            if len(res_dict)>0:\n",
    "                # To guard againt empty result dictionary created with no icesat2 passing the quality control above\n",
    "                # 1. Combine Dataframes for each of 6 ground-tracks into single Dataframe\n",
    "                count = 0\n",
    "                for k in res_dict.keys():\n",
    "                    if count == 0:\n",
    "                        df = res_dict[k]\n",
    "                        df['strip'] = k\n",
    "                        count += 1\n",
    "                    else:\n",
    "                        df1 = res_dict[k]\n",
    "                        df1['strip'] = k\n",
    "                        df = pd.concat([df, df1], axis=0)\n",
    "                # Choose filename for csv and shapefile\n",
    "                atl_fname = os.path.splitext(hdf_path)[0].split('/')[-1]\n",
    "                df.to_csv(f'{icesat2_path}/{atl_fname}.csv', index=False)                \n",
    "                # 2. Convert to Geopandas\n",
    "                df['geometry'] = df[['lon', 'lat']].apply(lambda x: Point(x), axis=1)\n",
    "                gdf = gpd.GeoDataFrame(df[['t_dt', 'h_li', 'q_flag', 'strip', 'geometry']], geometry='geometry', crs = 'EPSG:4326')\n",
    "                if gis_output=='shp':\n",
    "                    gdf['t_dt'] = gdf['t_dt'].dt.strftime('%Y-%m-%d %H:%M:%S.%f') #To prevent DriverSupportError: ESRI Shapefile does not support datetime fields\n",
    "                    gdf.to_file(f'{icesat2_path}/{atl_fname}.shp')\n",
    "                elif gis_output=='gpkg':\n",
    "                    gdf.to_file(f'{icesat2_path}/{atl_fname}.gpkg', driver='GPKG')                \n",
    "            else:\n",
    "                print(f\"No land ice data in this HDF file; csv or shp not created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function defined above to parse hdf files\n",
    "read_atl06(icesat2_path, 'gpkg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can analyze the data in any software of your choice  \n",
    "QGIS would be a good start but you can also use python, R, matlab"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
