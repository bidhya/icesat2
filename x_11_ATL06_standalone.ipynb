{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download ATL06 Data from NSIDC server\n",
    "Should work with any random area (either shapefile or coordinates of box)  \n",
    "Make changes to the codes following number 1 to 6 to fulfill your requirements  \n",
    "\n",
    "## Inputs:\n",
    "    - define the folder to download and process files\n",
    "    - shapefile with defined projection (can be be in any system)\n",
    "    - or coordinates of box [lowerleft, upper right] or coordinates in lat/lon coordinate system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "from datetime import timedelta\n",
    "\n",
    "#import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, Polygon #, mapping\n",
    "#from shapely.geometry.polygon import orient\n",
    "from statistics import mean\n",
    "import h5py\n",
    "import json\n",
    "import zipfile\n",
    "import io\n",
    "\n",
    "import requests\n",
    "import getpass\n",
    "import socket\n",
    "#from requests.auth import HTTPBasicAuth\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```If you get any error, that means some packages are missing, most likely geopandas and h5py   ```\n",
    "Install them from Anaconda navigator  \n",
    "or manually from command prompt/terminal as such  \n",
    "- conda install geopandas\n",
    "- conda install h5py\n",
    "\n",
    "If there is some error in installation, it means package not available in default anaconda package. use the following command  \n",
    "- conda install h5py -c conda-forge\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Choose output Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Housekeeping: This is where files will be downloaded and all ouputs saved\n",
    "# icesat2_path = 'D:/wspace/icesat2/problematic' #CHANGE THIS\n",
    "icesat2_path = 'test_data/test1' #CHANGE THIS\n",
    "if not os.path.exists(icesat2_path):\n",
    "    print(f'Create Output Directory : {icesat2_path}')\n",
    "    os.makedirs(icesat2_path) #exist_ok=True to prevent complaint if directory exist\n",
    "    os.mkdir(f'{icesat2_path}/downloads')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Either give a full path to shapefile or replace the corner coordinates below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x2b621a1c9590>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Either give the full path to outline as polygon shapefile\n",
    "shp = 'gis/aoi_greenland/aoi_for_icesat.shp' #insert full path to shapefile if you have one\n",
    "if shp:\n",
    "    gdf = gpd.read_file(shp)\n",
    "    #gdf = get_rema_strip_polygon(strip_folder, 'arctic')\n",
    "    # Convert to Latlong\n",
    "    gdf = gdf.to_crs({'init': 'epsg:4326'})\n",
    "else:\n",
    "    print('No shapefile give, replace the lower-left and upper-right coordinates to define the box')\n",
    "    # Or Digitize the outline of shapefile (replace the coordinates for your area here)\n",
    "    x0 = -147.5\n",
    "    y0 = 64.2\n",
    "    x1 = -146.5\n",
    "    y1 = 65.2\n",
    "#     x0 = -20.75\n",
    "#     y0 = 78.80\n",
    "#     x1 = -21.34\n",
    "#     y1 = 78.89\n",
    "    geom = Polygon([(x0, y0), (x1, y0), (x1, y1), (x0, y1)])\n",
    "    gdf = gpd.GeoSeries(geom, crs='epsg:4326')\n",
    "# Save for future use (but not required for script to work)    \n",
    "gdf.to_file(f'{icesat2_path}/outline.shp')\n",
    "shp_json = gdf.to_json() #use json as shapefile, or kml too, shp file not yet tried\n",
    "gdf.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.holoviews_exec.v0+json": "",
      "text/html": [
       "<div id='1300' style='display: table; margin: 0 auto;'>\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "  <div class=\"bk-root\" id=\"3c5ef23f-c8b1-47f6-bb3c-2da44784a519\" data-root-id=\"1300\"></div>\n",
       "</div>\n",
       "<script type=\"application/javascript\">(function(root) {\n",
       "  function embed_document(root) {\n",
       "    \n",
       "  var docs_json = {\"41a21bca-6d6f-48ca-bbdc-8105b8dea69a\":{\"roots\":{\"references\":[{\"attributes\":{\"align\":null,\"below\":[{\"id\":\"1309\",\"type\":\"LinearAxis\"}],\"center\":[{\"id\":\"1313\",\"type\":\"Grid\"},{\"id\":\"1318\",\"type\":\"Grid\"}],\"frame_height\":300,\"frame_width\":313,\"left\":[{\"id\":\"1314\",\"type\":\"LinearAxis\"}],\"margin\":null,\"match_aspect\":true,\"min_border_bottom\":10,\"min_border_left\":10,\"min_border_right\":10,\"min_border_top\":10,\"plot_height\":null,\"plot_width\":null,\"renderers\":[{\"id\":\"1354\",\"type\":\"TileRenderer\"},{\"id\":\"1364\",\"type\":\"GlyphRenderer\"}],\"sizing_mode\":\"fixed\",\"title\":{\"id\":\"1301\",\"type\":\"Title\"},\"toolbar\":{\"id\":\"1322\",\"type\":\"Toolbar\"},\"x_range\":{\"id\":\"1291\",\"type\":\"Range1d\"},\"x_scale\":{\"id\":\"1305\",\"type\":\"LinearScale\"},\"y_range\":{\"id\":\"1292\",\"type\":\"Range1d\"},\"y_scale\":{\"id\":\"1307\",\"type\":\"LinearScale\"}},\"id\":\"1300\",\"subtype\":\"Figure\",\"type\":\"Plot\"},{\"attributes\":{},\"id\":\"1310\",\"type\":\"BasicTicker\"},{\"attributes\":{\"data_source\":{\"id\":\"1358\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"1361\",\"type\":\"MultiPolygons\"},\"hover_glyph\":null,\"muted_glyph\":{\"id\":\"1363\",\"type\":\"MultiPolygons\"},\"nonselection_glyph\":{\"id\":\"1362\",\"type\":\"MultiPolygons\"},\"selection_glyph\":null,\"view\":{\"id\":\"1365\",\"type\":\"CDSView\"}},\"id\":\"1364\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"callback\":null,\"end\":15033189.47719394,\"min_interval\":5,\"reset_end\":15033189.47719394,\"reset_start\":14724835.123002423,\"start\":14724835.123002423,\"tags\":[[[\"Latitude\",\"Latitude\",null]]]},\"id\":\"1292\",\"type\":\"Range1d\"},{\"attributes\":{\"grid_line_color\":null,\"ticker\":{\"id\":\"1310\",\"type\":\"BasicTicker\"}},\"id\":\"1313\",\"type\":\"Grid\"},{\"attributes\":{\"dimension\":1,\"grid_line_color\":null,\"ticker\":{\"id\":\"1315\",\"type\":\"BasicTicker\"}},\"id\":\"1318\",\"type\":\"Grid\"},{\"attributes\":{\"level\":\"underlay\",\"tile_source\":{\"id\":\"1351\",\"type\":\"WMTSTileSource\"}},\"id\":\"1354\",\"type\":\"TileRenderer\"},{\"attributes\":{},\"id\":\"1307\",\"type\":\"LinearScale\"},{\"attributes\":{},\"id\":\"1315\",\"type\":\"BasicTicker\"},{\"attributes\":{\"axis_label\":\"Latitude\",\"bounds\":\"auto\",\"formatter\":{\"id\":\"1334\",\"type\":\"MercatorTickFormatter\"},\"major_label_orientation\":\"horizontal\",\"ticker\":{\"id\":\"1333\",\"type\":\"MercatorTicker\"}},\"id\":\"1314\",\"type\":\"LinearAxis\"},{\"attributes\":{\"axis_label\":\"Longitude\",\"bounds\":\"auto\",\"formatter\":{\"id\":\"1332\",\"type\":\"MercatorTickFormatter\"},\"major_label_orientation\":\"horizontal\",\"ticker\":{\"id\":\"1331\",\"type\":\"MercatorTicker\"}},\"id\":\"1309\",\"type\":\"LinearAxis\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.2},\"fill_color\":{\"value\":\"#1f77b3\"},\"line_alpha\":{\"value\":0.2},\"line_color\":{\"value\":\"black\"},\"xs\":{\"field\":\"xs\"},\"ys\":{\"field\":\"ys\"}},\"id\":\"1363\",\"type\":\"MultiPolygons\"},{\"attributes\":{\"text\":\"\",\"text_color\":{\"value\":\"black\"},\"text_font_size\":{\"value\":\"12pt\"}},\"id\":\"1301\",\"type\":\"Title\"},{\"attributes\":{\"dimension\":\"lon\"},\"id\":\"1331\",\"type\":\"MercatorTicker\"},{\"attributes\":{\"callback\":null,\"end\":-2248391.1337220697,\"min_interval\":5,\"reset_end\":-2248391.1337220697,\"reset_start\":-2570236.472226445,\"start\":-2570236.472226445,\"tags\":[[[\"Longitude\",\"Longitude\",null]]]},\"id\":\"1291\",\"type\":\"Range1d\"},{\"attributes\":{\"active_drag\":\"auto\",\"active_inspect\":\"auto\",\"active_multi\":null,\"active_scroll\":\"auto\",\"active_tap\":\"auto\",\"tools\":[{\"id\":\"1293\",\"type\":\"WheelZoomTool\"},{\"id\":\"1294\",\"type\":\"BoxZoomTool\"},{\"id\":\"1297\",\"type\":\"HoverTool\"},{\"id\":\"1320\",\"type\":\"PanTool\"},{\"id\":\"1321\",\"type\":\"ResetTool\"}]},\"id\":\"1322\",\"type\":\"Toolbar\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.5},\"fill_color\":{\"value\":\"#1f77b3\"},\"line_alpha\":{\"value\":0.5},\"line_color\":{\"value\":\"black\"},\"xs\":{\"field\":\"xs\"},\"ys\":{\"field\":\"ys\"}},\"id\":\"1362\",\"type\":\"MultiPolygons\"},{\"attributes\":{\"zoom_on_axis\":false},\"id\":\"1293\",\"type\":\"WheelZoomTool\"},{\"attributes\":{},\"id\":\"1320\",\"type\":\"PanTool\"},{\"attributes\":{},\"id\":\"1305\",\"type\":\"LinearScale\"},{\"attributes\":{\"bottom_units\":\"screen\",\"fill_alpha\":{\"value\":0.5},\"fill_color\":{\"value\":\"lightgrey\"},\"left_units\":\"screen\",\"level\":\"overlay\",\"line_alpha\":{\"value\":1.0},\"line_color\":{\"value\":\"black\"},\"line_dash\":[4,4],\"line_width\":{\"value\":2},\"render_mode\":\"css\",\"right_units\":\"screen\",\"top_units\":\"screen\"},\"id\":\"1348\",\"type\":\"BoxAnnotation\"},{\"attributes\":{\"attribution\":\"&copy; <a href=\\\"http://downloads.esri.com/ArcGISOnline/docs/tou_summary.pdf\\\">Esri</a>, Earthstar Geographics\",\"url\":\"https://server.arcgisonline.com/ArcGIS/rest/services/World_Imagery/MapServer/tile/{Z}/{Y}/{X}.jpg\"},\"id\":\"1351\",\"type\":\"WMTSTileSource\"},{\"attributes\":{\"dimension\":\"lon\"},\"id\":\"1332\",\"type\":\"MercatorTickFormatter\"},{\"attributes\":{},\"id\":\"1368\",\"type\":\"UnionRenderers\"},{\"attributes\":{},\"id\":\"1321\",\"type\":\"ResetTool\"},{\"attributes\":{\"dimension\":\"lat\"},\"id\":\"1333\",\"type\":\"MercatorTicker\"},{\"attributes\":{\"callback\":null,\"data\":{\"xs\":[[[{\"__ndarray__\":\"mQOTXIoKQ8FtFt6CYydBwS57wdiyl0HBIaKxSv6bQ8GZA5NcigpDwQ==\",\"dtype\":\"float64\",\"shape\":[5]}]],[[{\"__ndarray__\":\"sQm+3LPoQcH8F9oiX+hBwXru2SaL6EHBBt4jGa3oQcH1Nl7LLelBwbdiML9J6UHBhyHSeCzpQcHjgTNyzuhBwX8ffjHU6EHB4AmF5DboQcFEv5yqJ+hBwUOmTxMO6EHBp4Jy0efnQcE1yCuIpOdBwZoG2Y3g5kHBZ5VqYdLmQcHQAW3hieZBwbprQeSS5kHB5gtQ4H7mQcHjhg6jVeZBwWIZYZHH5UHBr9HKXWLlQcF7A2TkQuVBwduYBBl35EHBgpl7m4jkQcFBOxBAjuRBwWulsvFN5EHBfuKAITzkQcEqUdakp+RBwQ3SwV3R5EHBosqRQGnkQcHK6G0M9ONBwWoE7Nfb40HB0QRvGGHjQcFpzMV9quJBwaTN0P7k4UHBprxmiRzhQcGkwgsq8uBBwRm8eTee30HBfshvVdfdQcHT9fsVutxBwZEUcdNg3EHBnZS74dLaQcFzwKsCTNlBwcu6I1sI2UHBBr5nxHHYQcG6aH0vjtdBwY0QrMX51kHBmqB2alzWQcFyfHjnidVBwd1BkfV11EHB7EOyLVrUQcFTpYzb49RBwW2f8ojn1UHBnZXq3yvXQcGvmx6Q19hBwYVRPfDa2EHBqSny+DbYQcH/krgG4tZBwZkuxNZu1kHBBB3QZBXUQcH4ZmaoStRBwWKr/Kci1UHBOKoJFdjVQcEh8JfeedZBwZcjinvE10HBP+0VTTLZQcFFY4yKCNtBwZYtEWxw3UHBY10qBnneQcECO7P3EN5BwRiUFfCV3EHB66KmluPbQcGXRa5aJdtBwb0bRVwV2kHBXXF4JOjZQcGajoaq39pBwSzdRCMl3EHBBsxe2GreQcFRzEMODuFBwQms/h+14UHBtVlo3DXjQcHRUvt10ONBwdQtxUWy5EHB+2R0RRHlQcHoHDwx5OVBwfPLarhA6EHByQmstInoQcEbGVScfOlBwY6wwbQF60HBA6hx3QXsQcG0SYUwK+xBwfcFZdHZ60HBNczBWKLrQcFU3tjBwOpBwQRT7fdJ6UHBT86StvLmQcFgjdwP8uZBwRmQ6nEM6EHB1f5eYoboQcGTQVaaAulBwQ8UthSM6UHB4GKOysnrQcHFKBZiwu1Bwa9++Fdz7kHB+eFidVXvQcGhCx9Pg/BBweRsAZTX8EHBB0OMBn7wQcGoVlJBR/BBwQN1mMQU8EHBpojmoH7vQcGqWTQESO5BwaKag5Tl7EHBLMn3ixXsQcHWJ01T0upBwbEJvtyz6EHB\",\"dtype\":\"float64\",\"shape\":[117]}]]],\"ys\":[[[{\"__ndarray__\":\"z+0+wmysbEH46C/rfXtsQaH31lHcFWxBy/68rSBBbEHP7T7CbKxsQQ==\",\"dtype\":\"float64\",\"shape\":[5]}]],[[{\"__ndarray__\":\"fxuIzoJQbEH7eRlyjlBsQVQwoGO5UGxBH/LLM9pQbEFQCVF451BsQcqMRZj5UGxBYoc9ZxVRbEGWoNEFJFFsQZRMar9UUWxBcLTCVlVRbEGFC16qW1FsQdo/kN9wUWxBcbLVqn9RbEE5VH08iFFsQZxs94yMUWxBXlnb831RbEET0aLwhFFsQUImDACkUWxBUVbV1bxRbEEXTNtNzVFsQW3SE5XQUWxBHEhJWMFRbEH4bamnXVFsQWKuSBRbUWxBoPuDDGpRbEGeiaK8iFFsQWKR4KWYUWxB67wit6tRbEFwueCmuVFsQU+HhlbYUWxBVbOF1uVRbEHLQ03Pz1FsQV0qQhjAUWxBYxV4e7hRbEE98FdEvlFsQaM6t2C6UWxBBdFjLsFRbEFqefeM3VFsQVN4DOTjUWxB6BbLdOBRbEHLjzSnmFFsQWX5+8OdUWxBOWEkoaBRbEGN+CVAnVFsQfNV9LCIUWxBEDyX73dRbEECgaWncFFsQQT0UzZ+UWxBj1PjyZBRbEF933VtllFsQas5FsaTUWxBnb2yl29RbEHXZj/QU1FsQbDpVIwpUWxBQm8ZeQNRbEF150LZ1FBsQQkKAS/DUGxBCRLN+sJQbEEvihwfw1BsQbFnnEKlUGxBruOOXIFQbEFJ9b40YVBsQRJTejNfUGxBmuimWVxQbEGeHZlBPlBsQdJTs9b7T2xBEYsUZdlPbEGWR0SEq09sQeVxLLtYT2xBqpBdDkJPbEHNXQJvEE9sQYXSNlkET2xB1RuVgupObEEyuQBc105sQTSWtCfXTmxBbkjl/39ObEHC2cJgL05sQRAqenDtTWxBpTkD8J1NbEGxNbzUak1sQeGQS1M1TWxBU9gJASxNbEFDMQ8yZU1sQZIMCT1kTWxB37eBv0BNbEHySxYhNk1sQSMPbiEdTWxBrTNrQFdNbEEYLXsrWk1sQfoiQ6nNTGxB55LdCMBMbEEQOXUu6kxsQefmIDIzTWxBuamnE2lNbEGfhtMeoE1sQXIe1KsCTmxBrIjAXm5ObEG61I9Ujk5sQWgOBp+RTmxBMQu6JqdObEFEAPHxyE5sQdxurSvITmxBPD94sslObEGlWCWMyU5sQbbX+iTbTmxB8b0JMf5ObEF2d8iYEU9sQVMWRPVAT2xBwTSqHYFPbEHK86UZqU9sQd4JOIezT2xBLL+D7bZPbEGIZ6qYzE9sQUj9zmztT2xBI8AHcQhQbEEtZg67MlBsQX8biM6CUGxB\",\"dtype\":\"float64\",\"shape\":[117]}]]]},\"selected\":{\"id\":\"1359\",\"type\":\"Selection\"},\"selection_policy\":{\"id\":\"1368\",\"type\":\"UnionRenderers\"}},\"id\":\"1358\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"dimension\":\"lat\"},\"id\":\"1334\",\"type\":\"MercatorTickFormatter\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.5},\"fill_color\":{\"value\":\"#1f77b3\"},\"line_alpha\":{\"value\":0.5},\"xs\":{\"field\":\"xs\"},\"ys\":{\"field\":\"ys\"}},\"id\":\"1361\",\"type\":\"MultiPolygons\"},{\"attributes\":{},\"id\":\"1359\",\"type\":\"Selection\"},{\"attributes\":{\"match_aspect\":true,\"overlay\":{\"id\":\"1348\",\"type\":\"BoxAnnotation\"}},\"id\":\"1294\",\"type\":\"BoxZoomTool\"},{\"attributes\":{\"source\":{\"id\":\"1358\",\"type\":\"ColumnDataSource\"}},\"id\":\"1365\",\"type\":\"CDSView\"},{\"attributes\":{\"callback\":null,\"renderers\":[{\"id\":\"1364\",\"type\":\"GlyphRenderer\"}],\"tags\":[\"hv_created\"],\"tooltips\":null},\"id\":\"1297\",\"type\":\"HoverTool\"}],\"root_ids\":[\"1300\"]},\"title\":\"Bokeh Application\",\"version\":\"1.4.0\"}};\n",
       "  var render_items = [{\"docid\":\"41a21bca-6d6f-48ca-bbdc-8105b8dea69a\",\"roots\":{\"1300\":\"3c5ef23f-c8b1-47f6-bb3c-2da44784a519\"}}];\n",
       "  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
       "\n",
       "  }\n",
       "  if (root.Bokeh !== undefined) {\n",
       "    embed_document(root);\n",
       "  } else {\n",
       "    var attempts = 0;\n",
       "    var timer = setInterval(function(root) {\n",
       "      if (root.Bokeh !== undefined) {\n",
       "        clearInterval(timer);\n",
       "        embed_document(root);\n",
       "      } else {\n",
       "        attempts++;\n",
       "        if (attempts > 100) {\n",
       "          clearInterval(timer);\n",
       "          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\n",
       "        }\n",
       "      }\n",
       "    }, 10, root)\n",
       "  }\n",
       "})(window);</script>"
      ],
      "text/plain": [
       ":Overlay\n",
       "   .WMTS.I     :WMTS   [Longitude,Latitude]\n",
       "   .Polygons.I :Polygons   [Longitude,Latitude]"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "application/vnd.holoviews_exec.v0+json": {
       "id": "1300"
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import hvplot\n",
    "import hvplot.pandas\n",
    "import geoviews as gv\n",
    "\n",
    "# # Since geoseries can't be plotted by holoviews, convert to geodataframe and define the geometry\n",
    "# gdf1 = gpd.GeoDataFrame(gdf)\n",
    "# gdf1.columns = ['geometry']\n",
    "\n",
    "base = gv.tile_sources.ESRI\n",
    "\n",
    "strips = gdf.hvplot(geo=True, alpha=0.5, c=None)\n",
    "base * strips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'-23.08882806802458,78.64732924954261,-20.19764020101351,79.17986902177026'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#% To use Bounding Box for subsetting\n",
    "bounding_box = ','.join(map(str, list(gdf.total_bounds)))\n",
    "bounding_box\n",
    "# Bounding box subsetting (bbox) in same format as bounding_box\n",
    "#bbox = bounding_box #not used in post"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Temporal Subsetting\n",
    "Replace Start and End date time in the following string format by replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-14T00:00:00Z,2019-12-31T23:59:59Z\n"
     ]
    }
   ],
   "source": [
    "start_date = '2018-10-14' #yyyy-MM-dd format '2018-10-14' #'2019-04-01' # start of ATL06 2018/10/14\n",
    "start_time = '00:00:00' #HH:mm:ss format\n",
    "end_date = '2019-12-31' #'2019-09-30'\n",
    "end_time = '23:59:59'\n",
    "temporal = start_date + 'T' + start_time + 'Z' + ',' + end_date + 'T' + end_time + 'Z'\n",
    "print(temporal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# No need to change anything below except item 4 (your authentication)\n",
    "But observe the size of our ouputs and modify spatial/temporal extent accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Granules search parameters the NSIDC Server\n",
    "short_name = 'ATL06'\n",
    "latest_version = '002' #directly without using the cmr search to simplify the script\n",
    "search_params = {\n",
    "        'short_name': short_name,\n",
    "        'version': latest_version,\n",
    "        'temporal': temporal,\n",
    "        'page_size': 100,\n",
    "        'page_num': 1,\n",
    "        'bounding_box': bounding_box\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query number of granules using our (paging over results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Number of Granules = 0\n"
     ]
    }
   ],
   "source": [
    "granule_search_url = 'https://cmr.earthdata.nasa.gov/search/granules'\n",
    "headers={'Accept': 'application/json'} #if not defined in first block to get the token\n",
    "granules = []\n",
    "count = 0\n",
    "while True:\n",
    "    print(count)\n",
    "    response = requests.get(granule_search_url, params=search_params, headers=headers)\n",
    "    results = json.loads(response.content)\n",
    "    if len(results['feed']['entry']) == 0:\n",
    "        # Out of results, so break out of loop\n",
    "        break\n",
    "    # Collect results and increment page_num\n",
    "    granules.extend(results['feed']['entry'])\n",
    "    search_params['page_num'] += 1\n",
    "    count +=1\n",
    "# Get number of granules over my area and time of interest\n",
    "print(f'Number of Granules = {len(granules)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Size of Granules:  0 MB\n"
     ]
    }
   ],
   "source": [
    "granule_sizes = [float(granule['granule_size']) for granule in granules]\n",
    "print('Total Size of Granules: ', sum(granule_sizes), 'MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Choose the authentication mechanism\n",
    "username, password, and the token (by running the token code below (item 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success getting authentication\n"
     ]
    }
   ],
   "source": [
    "# Query service capability URL : seems only to see what service is availabe\n",
    "from xml.etree import ElementTree as ET\n",
    "capability_url = f'https://n5eil02u.ecs.nsidc.org/egi/capabilities/{short_name}.{latest_version}.xml'\n",
    "#print(capability_url)\n",
    "\n",
    "#Create session to store cookie and pass credentials to capabilities url\n",
    "email=''\n",
    "try:\n",
    "    from icesat2_search_and_download_ATL import get_api_key\n",
    "    uid, pswd, token = get_api_key()\n",
    "    print(\"Success getting authentication\")\n",
    "except:\n",
    "    # Input explicitly here\n",
    "    uid = ''#'Replace with your username'\n",
    "    email = '' #if you provide your email, you will receive email for every order; \n",
    "    pswd = ''#'Repalce with your password'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Request token from Common Metadata Repository using Earthdata credentials\n",
    "Run this block of code only once, note the printed token, and replace the token in the next block of code"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "token_api_url = 'https://cmr.earthdata.nasa.gov/legacy-services/rest/tokens'\n",
    "# hostname and ip may be not used!\n",
    "hostname = socket.gethostname() # get my personal computer name, eg staff-xxx-xxx; 'STAFF-BY-xx'\n",
    "ip = socket.gethostbyname(hostname)\n",
    "data = {\n",
    "    'token': {\n",
    "        'username': uid,\n",
    "        'password': pswd,\n",
    "        'client_id': 'NSIDC_client_id',\n",
    "        'user_ip_address': ip\n",
    "    }\n",
    "}\n",
    "headers={'Accept': 'application/json'}\n",
    "response = requests.post(token_api_url, json=data, headers=headers)\n",
    "token = json.loads(response.content)['token']['id']\n",
    "print(token) #use this token for a month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pswd = getpass.getpass('Earthdata Login password: ') #B2f\n",
    "# token = '' #'Replace this with the output above' \n",
    "\n",
    "session = requests.session()\n",
    "s = session.get(capability_url)\n",
    "\n",
    "response = session.get(s.url,auth=(uid,pswd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage = ''\n",
    "# Temporal subsetting KVP\n",
    "timevar = temporal # this seemed to work as well, when used temporal below directly\n",
    "# Request data from the NSIDC data access API.\n",
    "'''the API is structured as a URL with a base plus individual key-value-pairs (KVPs) separated by ‘&’. \n",
    "The base URL of the NSIDC API is: '''\n",
    "base_url = 'https://n5eil02u.ecs.nsidc.org/egi/request' ##Set NSIDC data access base URL\n",
    "\n",
    "# Set number of granules requested per order, which we will initially set to 10.\n",
    "page_size = 10\n",
    "#Determine number of pages basd on page_size and total granules. Loop requests by this value\n",
    "page_num = math.ceil(len(granules)/page_size)\n",
    "#Set request mode. \n",
    "request_mode = 'async' #with synchronous, there is possibility of timeout errors\n",
    "\n",
    "#Create config dictionary\n",
    "config_params = {\n",
    "    'request_mode': request_mode, \n",
    "    'page_size': page_size,  \n",
    "    'token': token, \n",
    "    'email': email,   \n",
    "}\n",
    "\n",
    "#timevar replaced with temporal: and it seems to work fine\n",
    "custom_params = {\n",
    "    'time': temporal,\n",
    "    'Coverage': coverage, \n",
    "    }\n",
    "# Creating final request parameter dictionary with search, config, and customization parameters.\n",
    "subset_request_params = {**search_params, **config_params, **custom_params}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This block of code will download the data  \n",
    "- may take a few minutes\n",
    "- you will receive email if you provided one\n",
    "- may print some error: these are mostly system error on NSIDC part (at least thats what I learned for Any at the AGU meeting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Choose method to make a request\n",
    "method = 'post' #'get' 'post'\n",
    "for i in range(page_num):\n",
    "    page_val = i + 1\n",
    "    print('Order: ', page_val)\n",
    "    if method == 'post':\n",
    "        # We are sending shapefile (in KML format) to the server\n",
    "        subset_request_params.update( {'page_num': page_val})\n",
    "        # Post polygon to API endpoint for polygon subsetting to subset based on original, non-simplified KML file\n",
    "        #shape_post = {'shapefile': open(kml_filepath, 'rb')}\n",
    "        shape_post = {'shapefile': shp_json}\n",
    "        \n",
    "        request = session.post(base_url, params=subset_request_params, files=shape_post, auth=(uid,pswd))\n",
    "\n",
    "    # Raise bad request: Loop will stop for bad response code.\n",
    "    request.raise_for_status()\n",
    "    #print('Order request URL: ', request.url)\n",
    "    esir_root = ET.fromstring(request.content)\n",
    "    #print('Order request response XML content: ', request.content)\n",
    "\n",
    "    #Look up order ID\n",
    "    orderlist = []\n",
    "    for order in esir_root.findall(\"./order/\"):\n",
    "        orderlist.append(order.text)\n",
    "    orderID = orderlist[0]\n",
    "    print('order ID: ', orderID)\n",
    "\n",
    "    #Create status URL\n",
    "    statusURL = base_url + '/' + orderID\n",
    "    print('status URL: ', statusURL)\n",
    "\n",
    "    #Find order status\n",
    "    request_response = session.get(statusURL)\n",
    "    #logging.info(f'Order Status HTTP response: {request_response.status_code}')\n",
    "\n",
    "    # Raise bad request: Loop will stop for bad response code.\n",
    "    request_response.raise_for_status()\n",
    "    request_root = ET.fromstring(request_response.content)\n",
    "    statuslist = []\n",
    "    for status in request_root.findall(\"./requestStatus/\"):\n",
    "        statuslist.append(status.text)\n",
    "    status = statuslist[0]\n",
    "    print('Data request ', page_val, ' is submitting...')\n",
    "    print('Initial request status is ', status)\n",
    "\n",
    "    #Continue loop while request is still processing\n",
    "    while status == 'pending' or status == 'processing': \n",
    "        print('Status is not complete. Trying again.')\n",
    "        sleep_sec = 10\n",
    "        time.sleep(sleep_sec)\n",
    "        loop_response = session.get(statusURL)\n",
    "        # Raise bad request: Loop will stop for bad response code.\n",
    "        loop_response.raise_for_status()\n",
    "        loop_root = ET.fromstring(loop_response.content)\n",
    "        #find status\n",
    "        statuslist = []\n",
    "        for status in loop_root.findall(\"./requestStatus/\"):\n",
    "            statuslist.append(status.text)\n",
    "        status = statuslist[0]\n",
    "        print('Retry request status is: ', status)\n",
    "        if status == 'pending' or status == 'processing':\n",
    "            continue\n",
    "    #Order can either complete, complete_with_errors, or fail:\n",
    "    # Provide complete_with_errors error message:\n",
    "    if status == 'complete_with_errors' or status == 'failed':\n",
    "        messagelist = []\n",
    "        for message in loop_root.findall(\"./processInfo/\"):\n",
    "            messagelist.append(message.text)\n",
    "        logging.error(f'error messages: {messagelist}')\n",
    "        #pprint.pprint(messagelist)\n",
    "\n",
    "    # Download zipped order if status is complete or complete_with_errors\n",
    "    #'https://n5eil02u.ecs.nsidc.org/esir/5000000402535/166238470/\n",
    "    #processed_ATL06_20181102070512_05290110_002_01.h5\n",
    "    if status == 'complete' or status == 'complete_with_errors':\n",
    "        downloadURL = 'https://n5eil02u.ecs.nsidc.org/esir/' + orderID + '.zip'\n",
    "        print('Beginning download of zipped output...')\n",
    "        zip_response = session.get(downloadURL)\n",
    "        # Raise bad request: Loop will stop for bad response code.\n",
    "        zip_response.raise_for_status()\n",
    "        with zipfile.ZipFile(io.BytesIO(zip_response.content)) as z:\n",
    "            z.extractall(f'{icesat2_path}/downloads')\n",
    "        print('Data request', page_val, 'is complete.')\n",
    "    else: print('Request failed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further processing\n",
    "a. Extract the zip files  \n",
    "b. Parse the HDF files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extract the downloaded HDF files and cleanup the downloaded folder\n",
    "def move_files_from_order(icesat2_path):\n",
    "    ''' Extract files from downloaded subfoder [ie one by orderid] \n",
    "        and move all hdf files in one location\n",
    "    '''\n",
    "    hdf_path = icesat2_path\n",
    "    for root, dirs, files in os.walk(f'{icesat2_path}/downloads', topdown=False):\n",
    "        for f in files:\n",
    "            if f.endswith('h5'):\n",
    "                try:\n",
    "                    shutil.move(os.path.join(root, f), hdf_path)\n",
    "                except:\n",
    "                    logging.error(\"Extraction Error (SHUTIL): perhaps file already exist\")\n",
    "    #os.rmdir(f'{icesat2_path}/downloads') #for empty\n",
    "    shutil.rmtree(f'{icesat2_path}/downloads')\n",
    "    \n",
    "# Cleanup : move the hdf files to base folder and deleted the \"downloads\" folder\n",
    "move_files_from_order(icesat2_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert/Parse the HDF files to csv file/shapefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['processed_ATL06_20181017053005_02840105_002_01.h5',\n",
       " 'processed_ATL06_20181026153716_04280103_002_01.h5',\n",
       " 'processed_ATL06_20181115040622_07260105_002_01.h5',\n",
       " 'processed_ATL06_20181119035757_07870105_002_01.h5',\n",
       " 'processed_ATL06_20181124141317_08700103_002_01.h5',\n",
       " 'processed_ATL06_20181128140455_09310103_002_01.h5',\n",
       " 'processed_ATL06_20181218023352_12290105_002_01.h5',\n",
       " 'processed_ATL06_20181227124101_13730103_002_01.h5',\n",
       " 'processed_ATL06_20190116010949_02840205_002_01.h5',\n",
       " 'processed_ATL06_20190120010133_03450205_002_01.h5',\n",
       " 'processed_ATL06_20190217233746_07870205_002_01.h5',\n",
       " 'processed_ATL06_20190318221349_12290205_002_01.h5',\n",
       " 'processed_ATL06_20190328082055_13730203_002_01.h5',\n",
       " 'processed_ATL06_20190416204947_02840305_002_01.h5',\n",
       " 'processed_ATL06_20190426065655_04280303_002_01.h5',\n",
       " 'processed_ATL06_20190515192547_07260305_002_01.h5',\n",
       " 'processed_ATL06_20190519191724_07870305_002_01.h5',\n",
       " 'processed_ATL06_20190525053252_08700303_002_01.h5',\n",
       " 'processed_ATL06_20190529052432_09310303_002_01.h5',\n",
       " 'processed_ATL06_20190617175325_12290305_002_01.h5',\n",
       " 'processed_ATL06_20190726023631_04280403_002_01.h5',\n",
       " 'processed_ATL06_20190814150529_07260405_002_01.h5',\n",
       " 'processed_ATL06_20190818145711_07870405_002_01.h5',\n",
       " 'processed_ATL06_20190824011241_08700403_002_01.h5',\n",
       " 'processed_ATL06_20190828010421_09310403_002_01.h5']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = os.listdir(icesat2_path)\n",
    "hdf_files = [f for f in files if f.endswith('.h5')]\n",
    "hdf_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% To Parse hdf file and convert csv and shapefile for analysis/visualization\n",
    "from astropy.time import Time\n",
    "def gps2dyr(time, offset = 0):\n",
    "    \"\"\" Converte GPS time to decimal years. Helper function\"\"\"\n",
    "    time = time + offset\n",
    "    gps_time = Time(time, format='gps')#.decimalyear\n",
    "    iso_time = Time(gps_time, format='iso')\n",
    "    iso_time = iso_time.value\n",
    "    # Conver to pandas datetime [not sure if it is utc]\n",
    "    dt = pd.to_datetime(iso_time)\n",
    "    return dt\n",
    "\n",
    "def read_atl06(icesat2_path):\n",
    "    \"\"\" Read 1 ATL06 file and output 6 reduced files.     \n",
    "        Extract variables of interest and separate the ATL06 file \n",
    "        into each beam (ground track) and ascending/descending orbits.\"\"\"\n",
    "    files = os.listdir(icesat2_path)\n",
    "    hdf_files = [f for f in files if f.endswith('.h5')]\n",
    "    for f in hdf_files:\n",
    "        hdf_path = f'{icesat2_path}/{f}'\n",
    "        res_dict = {}\n",
    "        meta_dict = {} #These will hold metadata required for scalars per ground-track\n",
    "        group = ['gt1l', 'gt1r', 'gt2l', 'gt2r', 'gt3l', 'gt3r']\n",
    "        qual_str_count = ''\n",
    "        # Loop trough beams\n",
    "        # Perhaps read file first, then loop through groups; should be faster\n",
    "        with h5py.File(hdf_path, 'r') as fi:\n",
    "            # subset group based on data\n",
    "            group = [g for g in list(fi.keys()) if g in group]\n",
    "            for k,g in enumerate(group):\n",
    "                try:\n",
    "                    # 1) Read in data for a single beam #\n",
    "                    lat = fi[f'/{g}/land_ice_segments/latitude'][:]\n",
    "                    lon = fi[f'/{g}/land_ice_segments/longitude'][:]\n",
    "                    h_li = fi[f'/{g}/land_ice_segments/h_li'][:] #nan\n",
    "                    #s_li = fi[f'/{g}/land_ice_segments/h_li_sigma'][:] #nan\n",
    "                    t_dt = fi[f'/{g}/land_ice_segments/delta_time'][:]\n",
    "                    q_flag = fi[f'/{g}/land_ice_segments/atl06_quality_summary'][:]\n",
    "                    t_ref = fi['/ancillary_data/atlas_sdp_gps_epoch'][:] #scalar 1 value; required for offset\n",
    "                    \n",
    "                    meta_dict['t_ref'] = t_ref #dictionary of metadata (will be used in future to investigate data)\n",
    "                    # 2) Make Pandas dataframe\n",
    "                    df = pd.DataFrame({'lon':lon, 'lat':lat, 'h_li': h_li, 'q_flag':q_flag, 't_dt':t_dt})\n",
    "                    #Convert GPS time to actual time using function\n",
    "                    df['t_dt'] = df['t_dt'].apply(gps2dyr, offset=t_ref[0])\n",
    "                    # Fill Nans for na-data and drop\n",
    "                    df.loc[df.h_li>3e38, 'h_li'] = np.nan\n",
    "                    df = df.dropna()\n",
    "                    all_points = len(df)\n",
    "                    df = df[df.q_flag==0] #select only flags of zero (good values) can be empty sometimes\n",
    "                    good_quality_points = len(df) #len(df[df.q_flag==0])\n",
    "                    qual_str_count = qual_str_count + f'{g}={good_quality_points}/{all_points}; '\n",
    "                    if len(df)>0:\n",
    "                        # Assemble ground track into a dictionary, later we convert to csv and shp through df\n",
    "                        res_dict[g] = df\n",
    "                except:\n",
    "                    # Most like this error is due to empty dataframe \n",
    "                    # may not exist anymore since we are no dropping the bad quality data\n",
    "                    print(f'\\tException in reading hdf group (ground track), df length = {len(df)}')\n",
    "            #----------------------------------------------------------------------------------------------\n",
    "            # Now that ATL06 data from separate ground tracks are in one dict, merge it to df and save to csv/shp\n",
    "            if len(res_dict)>0:\n",
    "                print(f'\\tGood/Total Points: {qual_str_count}   Total GTs = {len(res_dict)}')\n",
    "                # To guard againt empty result dictionary created with no icesat2 passing the quality control above\n",
    "                # 1. Combine Dataframes for each of 6 ground-tracks into single Dataframe\n",
    "                count = 0\n",
    "                for k in res_dict.keys():\n",
    "                    # k = 'gt1l', 'gt1r' etc\n",
    "                    if count == 0:\n",
    "                        df = res_dict[k]\n",
    "                        df['strip'] = k\n",
    "                        count += 1\n",
    "                    else:\n",
    "                        df1 = res_dict[k]\n",
    "                        df1['strip'] = k\n",
    "                        df = pd.concat([df, df1], axis=0)\n",
    "                # Log the time range of icesat2 data (could be useful for understanding why some data is large)\n",
    "                # This may be creating exception when empty\n",
    "                time_range = df.t_dt.max() - df.t_dt.min()\n",
    "                print(f\"\\tRows = {len(df)} \\t Time Range = {time_range.total_seconds()} seconds\")\n",
    "                # Choose filename for csv and shapefile\n",
    "                atl_fname = os.path.splitext(hdf_path)[0].split('/')[-1]\n",
    "                #df = df[df.q_flag==0] # Already done above for each ground track\n",
    "                df.to_csv(f'{icesat2_path}/{atl_fname}.csv', index=False)\n",
    "                \n",
    "                # 2. Convert to Geopandas\n",
    "                df['coord'] = df[['lon', 'lat']].apply(lambda x: Point(x), axis=1)\n",
    "                gdf = gpd.GeoDataFrame(df[['t_dt', 'h_li', 'q_flag', 'strip', 'coord']], geometry='coord')\n",
    "                gdf['t_dt'] = gdf['t_dt'].dt.strftime('%Y-%m-%d %H:%M:%S.%f') #To prevent DriverSupportError: ESRI Shapefile does not support datetime fields\n",
    "                gdf.crs = {'init': 'epsg:4326'} #not yet verified or checked with what ICESAT-2 metadata provides\n",
    "                gdf.to_file(f'{icesat2_path}/{atl_fname}.shp')\n",
    "            else:\n",
    "                print(f\"\\tNo good quality Ground Track in this HDF file; csv or shp not created\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function defined above to parse hdf files\n",
    "read_atl06(icesat2_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can analyze the data in any software of your choice  \n",
    "QGIS would be a good start but you can also use python, R, matlab"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
