{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download ATL06 Data from NSIDC server\n",
    "Should work with any random area (either shapefile or coordinates of box)  \n",
    "Make changes to the codes following number 1 to 6 to fulfill your requirements  \n",
    "\n",
    "## Inputs:\n",
    "    - define the folder to download and process files\n",
    "    - shapefile with defined projection (can be be in any system)\n",
    "    - or coordinates of box [lowerleft, upper right] or coordinates in lat/lon coordinate system\n",
    "    \n",
    "Last Update: Sept 21, 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "from datetime import timedelta\n",
    "\n",
    "#import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, Polygon #, mapping\n",
    "#from shapely.geometry.polygon import orient\n",
    "from statistics import mean\n",
    "import h5py\n",
    "import json\n",
    "import zipfile\n",
    "import io\n",
    "\n",
    "import requests\n",
    "import getpass\n",
    "import socket\n",
    "#from requests.auth import HTTPBasicAuth\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```If you get any error, that means some packages are missing, most likely geopandas and h5py   ```\n",
    "Install them from Anaconda navigator  \n",
    "or manually from command prompt/terminal as such  \n",
    "- conda install geopandas\n",
    "- conda install h5py\n",
    "\n",
    "If there is some error in installation, it means package not available in default anaconda package. use the following command  \n",
    "- conda install h5py -c conda-forge\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Choose output Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create Output Directory : test_data\n"
     ]
    }
   ],
   "source": [
    "# Housekeeping: This is where files will be downloaded and all ouputs saved\n",
    "# icesat2_path = 'D:/wspace/icesat2/problematic' #CHANGE THIS\n",
    "icesat2_path = 'test_data' #CHANGE THIS\n",
    "if not os.path.exists(icesat2_path):\n",
    "    print(f'Create Output Directory : {icesat2_path}')\n",
    "    os.makedirs(icesat2_path) #exist_ok=True to prevent complaint if directory exist\n",
    "    os.mkdir(f'{icesat2_path}/downloads')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Either give a full path to shapefile or replace the corner coordinates below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No shapefile give, replace the lower-left and upper-right coordinates to define the box\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Either give the full path to outline as polygon shapefile\n",
    "shp = ''#'gis/aoi_greenland/aoi_for_icesat.shp' #insert full path to shapefile if you have one\n",
    "if shp:\n",
    "    gdf = gpd.read_file(shp)\n",
    "    #gdf = get_rema_strip_polygon(strip_folder, 'arctic')\n",
    "    # Convert to Latlong\n",
    "    gdf = gdf.to_crs('epsg:4326') #use this format if error due to old geopandas version {'init': 'epsg:4326'}\n",
    "else:\n",
    "    print('No shapefile give, replace the lower-left and upper-right coordinates to define the box')\n",
    "    # Or Digitize the outline of shapefile (replace the coordinates for your area here)\n",
    "    x0 = 100 # -147.5\n",
    "    y0 = -66 #64.2\n",
    "    x1 = 101 #-146.5\n",
    "    y1 = -67 #65.2\n",
    "#     x0 = -20.75\n",
    "#     y0 = 78.80\n",
    "#     x1 = -21.34\n",
    "#     y1 = 78.89\n",
    "    geom = Polygon([(x0, y0), (x1, y0), (x1, y1), (x0, y1)])\n",
    "    gdf = gpd.GeoSeries(geom, crs='epsg:4326')\n",
    "# Save for future use (but not required for script to work)    \n",
    "gdf.to_file(f'{icesat2_path}/outline.shp')\n",
    "shp_json = gdf.to_json() #use json as shapefile, or kml too, shp file not yet tried\n",
    "gdf.plot()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import hvplot\n",
    "import hvplot.pandas\n",
    "import geoviews as gv\n",
    "\n",
    "# # Since geoseries can't be plotted by holoviews, convert to geodataframe and define the geometry\n",
    "# gdf1 = gpd.GeoDataFrame(gdf)\n",
    "# gdf1.columns = ['geometry']\n",
    "\n",
    "base = gv.tile_sources.ESRI\n",
    "\n",
    "strips = gdf.hvplot(geo=True, alpha=0.5, c=None)\n",
    "base * strips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'100.0,-67.0,101.0,-66.0'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#% To use Bounding Box for subsetting\n",
    "bounding_box = ','.join(map(str, list(gdf.total_bounds)))\n",
    "bounding_box\n",
    "# Bounding box subsetting (bbox) in same format as bounding_box\n",
    "#bbox = bounding_box #not used in post"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Temporal Subsetting\n",
    "Replace Start and End date time in the following string format by replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-01T00:00:00Z,2020-08-31T23:59:59Z\n"
     ]
    }
   ],
   "source": [
    "start_date = '2020-05-01' #yyyy-MM-dd format '2018-10-14' #'2019-04-01' # start of ATL06 2018/10/14\n",
    "start_time = '00:00:00' #HH:mm:ss format\n",
    "end_date = '2020-08-31' #'2019-09-30'\n",
    "end_time = '23:59:59'\n",
    "temporal = start_date + 'T' + start_time + 'Z' + ',' + end_date + 'T' + end_time + 'Z'\n",
    "print(temporal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# No need to change anything below except item 4 (your authentication)\n",
    "But observe the size of our ouputs and modify spatial/temporal extent accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Granules search parameters the NSIDC Server\n",
    "short_name = 'ATL06'\n",
    "latest_version = '003' #directly without using the cmr search to simplify the script\n",
    "search_params = {\n",
    "        'short_name': short_name,\n",
    "        'version': latest_version,\n",
    "        'temporal': temporal,\n",
    "        'page_size': 100,\n",
    "        'page_num': 1,\n",
    "        'bounding_box': bounding_box\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query number of granules using our (paging over results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "Number of Granules = 13\n"
     ]
    }
   ],
   "source": [
    "granule_search_url = 'https://cmr.earthdata.nasa.gov/search/granules'\n",
    "headers={'Accept': 'application/json'} #if not defined in first block to get the token\n",
    "granules = []\n",
    "count = 0\n",
    "while True:\n",
    "    print(count)\n",
    "    response = requests.get(granule_search_url, params=search_params, headers=headers)\n",
    "    results = json.loads(response.content)\n",
    "    if len(results['feed']['entry']) == 0:\n",
    "        # Out of results, so break out of loop\n",
    "        break\n",
    "    # Collect results and increment page_num\n",
    "    granules.extend(results['feed']['entry'])\n",
    "    search_params['page_num'] += 1\n",
    "    count +=1\n",
    "# Get number of granules over my area and time of interest\n",
    "print(f'Number of Granules = {len(granules)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Size of Granules:  818.5789766311002 MB\n"
     ]
    }
   ],
   "source": [
    "granule_sizes = [float(granule['granule_size']) for granule in granules]\n",
    "print('Total Size of Granules: ', sum(granule_sizes), 'MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Choose the authentication mechanism\n",
    "username, password, and the token (by running the token code below (item 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success getting authentication\n"
     ]
    }
   ],
   "source": [
    "# Query service capability URL : seems only to see what service is availabe\n",
    "from xml.etree import ElementTree as ET\n",
    "capability_url = f'https://n5eil02u.ecs.nsidc.org/egi/capabilities/{short_name}.{latest_version}.xml'\n",
    "#print(capability_url)\n",
    "\n",
    "#Create session to store cookie and pass credentials to capabilities url\n",
    "email=''\n",
    "try:\n",
    "    from icesat2_download import get_api_key\n",
    "    uid, pswd, token = get_api_key()\n",
    "    print(\"Success getting authentication\")\n",
    "except:\n",
    "    # Input explicitly here\n",
    "    uid = ''#'Replace with your username'\n",
    "    email = '' #if you provide your email, you will receive email for every order; \n",
    "    pswd = ''#'Repalce with your password'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Request token from Common Metadata Repository using Earthdata credentials\n",
    "Run this block of code only once, note the printed token, and replace the token in the next block of code"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "token_api_url = 'https://cmr.earthdata.nasa.gov/legacy-services/rest/tokens'\n",
    "# hostname and ip may be not used!\n",
    "hostname = socket.gethostname() # get my personal computer name, eg staff-xxx-xxx; 'STAFF-BY-xx'\n",
    "ip = socket.gethostbyname(hostname)\n",
    "data = {\n",
    "    'token': {\n",
    "        'username': uid,\n",
    "        'password': pswd,\n",
    "        'client_id': 'NSIDC_client_id',\n",
    "        'user_ip_address': ip\n",
    "    }\n",
    "}\n",
    "headers={'Accept': 'application/json'}\n",
    "response = requests.post(token_api_url, json=data, headers=headers)\n",
    "token = json.loads(response.content)['token']['id']\n",
    "print(token) #use this token for a month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pswd = getpass.getpass('Earthdata Login password: ')\n",
    "# token = '' #'Replace this with the output above' \n",
    "\n",
    "session = requests.session()\n",
    "s = session.get(capability_url)\n",
    "\n",
    "response = session.get(s.url,auth=(uid,pswd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage = ''\n",
    "# Temporal subsetting KVP\n",
    "timevar = temporal # this seemed to work as well, when used temporal below directly\n",
    "# Request data from the NSIDC data access API.\n",
    "'''the API is structured as a URL with a base plus individual key-value-pairs (KVPs) separated by ‘&’. \n",
    "The base URL of the NSIDC API is: '''\n",
    "base_url = 'https://n5eil02u.ecs.nsidc.org/egi/request' ##Set NSIDC data access base URL\n",
    "\n",
    "# Set number of granules requested per order, which we will initially set to 10.\n",
    "page_size = 10\n",
    "#Determine number of pages basd on page_size and total granules. Loop requests by this value\n",
    "page_num = math.ceil(len(granules)/page_size)\n",
    "#Set request mode. \n",
    "request_mode = 'async' #with synchronous, there is possibility of timeout errors\n",
    "\n",
    "#Create config dictionary\n",
    "config_params = {\n",
    "    'request_mode': request_mode, \n",
    "    'page_size': page_size,  \n",
    "    'token': token, \n",
    "    'email': email,   \n",
    "}\n",
    "\n",
    "#timevar replaced with temporal: and it seems to work fine\n",
    "custom_params = {\n",
    "    'time': temporal,\n",
    "    'Coverage': coverage, \n",
    "    }\n",
    "# Creating final request parameter dictionary with search, config, and customization parameters.\n",
    "subset_request_params = {**search_params, **config_params, **custom_params}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This block of code will download the data  \n",
    "- may take a few minutes\n",
    "- you will receive email if you provided one\n",
    "- may print some error: these are mostly system error on NSIDC part (at least thats what I learned for Any at the AGU meeting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Order:  1\n",
      "order ID:  5000000874410\n",
      "status URL:  https://n5eil02u.ecs.nsidc.org/egi/request/5000000874410\n",
      "Data request  1  is submitting...\n",
      "Initial request status is  processing\n",
      "Status is not complete. Trying again.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:error messages: ['183000997:NoMatchingData - No data found that matched subset constraints. Exit code 3.', '187468885:NoMatchingData - No data found that matched subset constraints. Exit code 3.', '187496665:NoMatchingData - No data found that matched subset constraints. Exit code 3.', '187518114:NoMatchingData - No data found that matched subset constraints. Exit code 3.', 'PT12.912S', 'ICESAT2']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retry request status is:  complete_with_errors\n",
      "Beginning download of zipped output...\n",
      "Data request 1 is complete.\n",
      "Order:  2\n",
      "order ID:  5000000874412\n",
      "status URL:  https://n5eil02u.ecs.nsidc.org/egi/request/5000000874412\n",
      "Data request  2  is submitting...\n",
      "Initial request status is  processing\n",
      "Status is not complete. Trying again.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:error messages: ['187833236:NoMatchingData - No data found that matched subset constraints. Exit code 3.', '187829192:NoMatchingData - No data found that matched subset constraints. Exit code 3.', 'PT8.884S', 'ICESAT2']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retry request status is:  complete_with_errors\n",
      "Beginning download of zipped output...\n",
      "Data request 2 is complete.\n"
     ]
    }
   ],
   "source": [
    "#%% Choose method to make a request\n",
    "method = 'post' #'get' 'post'\n",
    "for i in range(page_num):\n",
    "    page_val = i + 1\n",
    "    print('Order: ', page_val)\n",
    "    if method == 'post':\n",
    "        # We are sending shapefile (in KML format) to the server\n",
    "        subset_request_params.update( {'page_num': page_val})\n",
    "        # Post polygon to API endpoint for polygon subsetting to subset based on original, non-simplified KML file\n",
    "        #shape_post = {'shapefile': open(kml_filepath, 'rb')}\n",
    "        shape_post = {'shapefile': shp_json}\n",
    "        \n",
    "        request = session.post(base_url, params=subset_request_params, files=shape_post, auth=(uid,pswd))\n",
    "\n",
    "    # Raise bad request: Loop will stop for bad response code.\n",
    "    request.raise_for_status()\n",
    "    #print('Order request URL: ', request.url)\n",
    "    esir_root = ET.fromstring(request.content)\n",
    "    #print('Order request response XML content: ', request.content)\n",
    "\n",
    "    #Look up order ID\n",
    "    orderlist = []\n",
    "    for order in esir_root.findall(\"./order/\"):\n",
    "        orderlist.append(order.text)\n",
    "    orderID = orderlist[0]\n",
    "    print('order ID: ', orderID)\n",
    "\n",
    "    #Create status URL\n",
    "    statusURL = base_url + '/' + orderID\n",
    "    print('status URL: ', statusURL)\n",
    "\n",
    "    #Find order status\n",
    "    request_response = session.get(statusURL)\n",
    "    #logging.info(f'Order Status HTTP response: {request_response.status_code}')\n",
    "\n",
    "    # Raise bad request: Loop will stop for bad response code.\n",
    "    request_response.raise_for_status()\n",
    "    request_root = ET.fromstring(request_response.content)\n",
    "    statuslist = []\n",
    "    for status in request_root.findall(\"./requestStatus/\"):\n",
    "        statuslist.append(status.text)\n",
    "    status = statuslist[0]\n",
    "    print('Data request ', page_val, ' is submitting...')\n",
    "    print('Initial request status is ', status)\n",
    "\n",
    "    #Continue loop while request is still processing\n",
    "    while status == 'pending' or status == 'processing': \n",
    "        print('Status is not complete. Trying again.')\n",
    "        sleep_sec = 20\n",
    "        time.sleep(sleep_sec)\n",
    "        loop_response = session.get(statusURL)\n",
    "        # Raise bad request: Loop will stop for bad response code.\n",
    "        loop_response.raise_for_status()\n",
    "        loop_root = ET.fromstring(loop_response.content)\n",
    "        #find status\n",
    "        statuslist = []\n",
    "        for status in loop_root.findall(\"./requestStatus/\"):\n",
    "            statuslist.append(status.text)\n",
    "        status = statuslist[0]\n",
    "        print('Retry request status is: ', status)\n",
    "        if status == 'pending' or status == 'processing':\n",
    "            continue\n",
    "    #Order can either complete, complete_with_errors, or fail:\n",
    "    # Provide complete_with_errors error message:\n",
    "    if status == 'complete_with_errors' or status == 'failed':\n",
    "        messagelist = []\n",
    "        for message in loop_root.findall(\"./processInfo/\"):\n",
    "            messagelist.append(message.text)\n",
    "        logging.error(f'error messages: {messagelist}')\n",
    "        #pprint.pprint(messagelist)\n",
    "\n",
    "    # Download zipped order if status is complete or complete_with_errors\n",
    "    #'https://n5eil02u.ecs.nsidc.org/esir/5000000402535/166238470/\n",
    "    #processed_ATL06_20181102070512_05290110_002_01.h5\n",
    "    if status == 'complete' or status == 'complete_with_errors':\n",
    "        downloadURL = 'https://n5eil02u.ecs.nsidc.org/esir/' + orderID + '.zip'\n",
    "        print('Beginning download of zipped output...')\n",
    "        zip_response = session.get(downloadURL)\n",
    "        # Raise bad request: Loop will stop for bad response code.\n",
    "        zip_response.raise_for_status()\n",
    "        with zipfile.ZipFile(io.BytesIO(zip_response.content)) as z:\n",
    "            z.extractall(f'{icesat2_path}/downloads')\n",
    "        print('Data request', page_val, 'is complete.')\n",
    "    else: print('Request failed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further processing\n",
    "a. Extract the zip files  \n",
    "b. Parse the HDF files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extract the downloaded HDF files and cleanup the downloaded folder\n",
    "def move_files_from_order(icesat2_path):\n",
    "    ''' Extract files from downloaded subfoder [ie one by orderid] \n",
    "        and move all hdf files in one location\n",
    "    '''\n",
    "    hdf_path = icesat2_path\n",
    "    for root, dirs, files in os.walk(f'{icesat2_path}/downloads', topdown=False):\n",
    "        for f in files:\n",
    "            if f.endswith('h5'):\n",
    "                try:\n",
    "                    shutil.move(os.path.join(root, f), hdf_path)\n",
    "                except:\n",
    "                    logging.error(\"Extraction Error (SHUTIL): perhaps file already exist\")\n",
    "    #os.rmdir(f'{icesat2_path}/downloads') #for empty\n",
    "    shutil.rmtree(f'{icesat2_path}/downloads')\n",
    "    \n",
    "# Cleanup : move the hdf files to base folder and deleted the \"downloads\" folder\n",
    "move_files_from_order(icesat2_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert/Parse the HDF files to csv file/shapefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['processed_ATL06_20200510054338_06820712_003_01.h5',\n",
       " 'processed_ATL06_20200521163056_08570710_003_01.h5',\n",
       " 'processed_ATL06_20200525162235_09180710_003_01.h5',\n",
       " 'processed_ATL06_20200623145836_13600710_003_01.h5',\n",
       " 'processed_ATL06_20200608041938_11240712_003_01.h5',\n",
       " 'processed_ATL06_20200612041117_11850712_003_01.h5',\n",
       " 'processed_ATL06_20200707025538_01790812_003_01.h5']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = os.listdir(icesat2_path)\n",
    "hdf_files = [f for f in files if f.endswith('.h5')]\n",
    "hdf_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% To Parse hdf file and convert csv and shapefile for analysis/visualization\n",
    "from astropy.time import Time\n",
    "def gps2dyr(time, offset = 0):\n",
    "    \"\"\" Converte GPS time to decimal years. Helper function\"\"\"\n",
    "    time = time + offset\n",
    "    gps_time = Time(time, format='gps')#.decimalyear\n",
    "    iso_time = Time(gps_time, format='iso')\n",
    "    iso_time = iso_time.value\n",
    "    # Conver to pandas datetime [not sure if it is utc]\n",
    "    dt = pd.to_datetime(iso_time)\n",
    "    return dt\n",
    "\n",
    "def read_atl06(icesat2_path, gis_output='shp'):\n",
    "    \"\"\" Read 1 ATL06 file and output 6 reduced files.     \n",
    "        Extract variables of interest and separate the ATL06 file \n",
    "        into each beam (ground track) and ascending/descending orbits.\"\"\"\n",
    "    files = os.listdir(icesat2_path)\n",
    "    hdf_files = [f for f in files if f.endswith('.h5') and 'ATL06' in f]\n",
    "    logging.info(f'Number of HDF files: {len(hdf_files)}')\n",
    "    for f in hdf_files:\n",
    "        logging.info(f'Parsing HDF file: {f}')\n",
    "        hdf_path = f'{icesat2_path}/{f}'\n",
    "        res_dict = {}\n",
    "        meta_dict = {} #These will hold metadata required for scalars per ground-track\n",
    "        group = ['gt1l', 'gt1r', 'gt2l', 'gt2r', 'gt3l', 'gt3r']\n",
    "        qual_str_count = ''\n",
    "        # Loop trough beams\n",
    "        # Perhaps read file first, then loop through groups; should be faster\n",
    "        with h5py.File(hdf_path, 'r') as fi:\n",
    "            # subset group based on data\n",
    "            group = [g for g in list(fi.keys()) if g in group]\n",
    "            group1 = len(group)\n",
    "            group = [g for g in group if 'land_ice_segments' in fi[f'/{g}']]\n",
    "            group2 = len(group)\n",
    "            if group2<group1:\n",
    "                print(f'Non-empty groups: {group2}/{group1}')\n",
    "            # NB: Assert if at least one group present else may be error due to enumeration\n",
    "            if len(group) == 0:\n",
    "                print('No any Group! ie, no land ice data. Halting further processing for this granule')\n",
    "                continue\n",
    "            for k,g in enumerate(group):\n",
    "                #if 'land_ice_segments' in fi[f'/{g}']:\n",
    "                # 1) Read in data for a single beam #\n",
    "                lat = fi[f'/{g}/land_ice_segments/latitude'][:]\n",
    "                lon = fi[f'/{g}/land_ice_segments/longitude'][:]\n",
    "                h_li = fi[f'/{g}/land_ice_segments/h_li'][:] #nan\n",
    "                #s_li = fi[f'/{g}/land_ice_segments/h_li_sigma'][:] #nan\n",
    "                t_dt = fi[f'/{g}/land_ice_segments/delta_time'][:]\n",
    "                q_flag = fi[f'/{g}/land_ice_segments/atl06_quality_summary'][:]\n",
    "                t_ref = fi['/ancillary_data/atlas_sdp_gps_epoch'][:] #scalar 1 value; required for offset\n",
    "                \n",
    "                meta_dict['t_ref'] = t_ref #dictionary of metadata (will be used in future to investigate data)\n",
    "                # 2) Make Pandas dataframe\n",
    "                df = pd.DataFrame({'lon':lon, 'lat':lat, 'h_li': h_li, 'q_flag':q_flag, 't_dt':t_dt})\n",
    "                #Convert GPS time to actual time using function\n",
    "                df['t_dt'] = df['t_dt'].apply(gps2dyr, offset=t_ref[0])\n",
    "                # Fill Nans for na-data and drop\n",
    "                df.loc[df.h_li>3e38, 'h_li'] = np.nan\n",
    "                df = df.dropna()\n",
    "                if len(df)>0:\n",
    "                    # Assemble ground track into a dictionary, later we convert to csv and shp through df\n",
    "                    res_dict[g] = df\n",
    "            #----------------------------------------------------------------------------------------------\n",
    "            # Now that ATL06 data from separate ground tracks are in one dict, merge it to df and save to csv/shp\n",
    "            if len(res_dict)>0:\n",
    "                # To guard againt empty result dictionary created with no icesat2 passing the quality control above\n",
    "                # 1. Combine Dataframes for each of 6 ground-tracks into single Dataframe\n",
    "                count = 0\n",
    "                for k in res_dict.keys():\n",
    "                    if count == 0:\n",
    "                        df = res_dict[k]\n",
    "                        df['strip'] = k\n",
    "                        count += 1\n",
    "                    else:\n",
    "                        df1 = res_dict[k]\n",
    "                        df1['strip'] = k\n",
    "                        df = pd.concat([df, df1], axis=0)\n",
    "                # Choose filename for csv and shapefile\n",
    "                atl_fname = os.path.splitext(hdf_path)[0].split('/')[-1]\n",
    "                df.to_csv(f'{icesat2_path}/{atl_fname}.csv', index=False)                \n",
    "                # 2. Convert to Geopandas\n",
    "                df['geometry'] = df[['lon', 'lat']].apply(lambda x: Point(x), axis=1)\n",
    "                gdf = gpd.GeoDataFrame(df[['t_dt', 'h_li', 'q_flag', 'strip', 'geometry']], geometry='geometry', crs = 'EPSG:4326')\n",
    "                if gis_output=='shp':\n",
    "                    gdf['t_dt'] = gdf['t_dt'].dt.strftime('%Y-%m-%d %H:%M:%S.%f') #To prevent DriverSupportError: ESRI Shapefile does not support datetime fields\n",
    "                    gdf.to_file(f'{icesat2_path}/{atl_fname}.shp')\n",
    "                elif gis_output=='gpkg':\n",
    "                    gdf.to_file(f'{icesat2_path}/{atl_fname}.gpkg', driver='GPKG')                \n",
    "            else:\n",
    "                print(f\"No land ice data in this HDF file; csv or shp not created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-empty groups: 4/6\n"
     ]
    }
   ],
   "source": [
    "# Call the function defined above to parse hdf files\n",
    "read_atl06(icesat2_path, 'gpkg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can analyze the data in any software of your choice  \n",
    "QGIS would be a good start but you can also use python, R, matlab"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
